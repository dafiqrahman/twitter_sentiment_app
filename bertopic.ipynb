{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic import BERTopic\n",
    "import matplotlib.pyplot as plt\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"firqaaa/indo-sentence-bert-base\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=2, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "stopwords = pd.read_csv(\"assets/stopwordbahasa.csv\", header=None)\n",
    "stopwords = stopwords[0].tolist()\n",
    "more_stopword = [\"ga\",\"iya\",\"dg\",'dengan', 'ia','bahwa','oleh',\"sy\",\"kl\",\"gak\",\"ah\",\"apa\",\"kok\",\"mau\",\"yg\",\"pak\",\"bapak\",\"ibu\",\"krn\",\"nya\",\"ya\"]\n",
    "stopwords = stopwords + more_stopword + list(string.punctuation)\n",
    "vectorizer_model = CountVectorizer(stop_words= stopwords)\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "# All steps together\n",
    "topic_model = BERTopic(\n",
    "  embedding_model=embedding_model,    # Step 1 - Extract embeddings\n",
    "  umap_model=umap_model,              # Step 2 - Reduce dimensionality\n",
    "  hdbscan_model=hdbscan_model,        # Step 3 - Cluster reduced embeddings\n",
    "  vectorizer_model=vectorizer_model,  # Step 4 - Tokenize topics\n",
    "  ctfidf_model=ctfidf_model,          # Step 5 - Extract topic words\n",
    "  diversity=0.5                       # Step 6 - Diversify topic words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saya sedang memakan buah apel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>saya sedang memakan buah jeruk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>andi sedang bermain bola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>budi suka makan mie ayam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sering banget makan bakso di tempat ini</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      text\n",
       "0            saya sedang memakan buah apel\n",
       "1           saya sedang memakan buah jeruk\n",
       "2                 andi sedang bermain bola\n",
       "3                 budi suka makan mie ayam\n",
       "4  sering banget makan bakso di tempat ini"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"saya sedang memakan buah apel\",\n",
    "             \"saya sedang memakan buah jeruk\",\n",
    "             \"andi sedang bermain bola\",\n",
    "             \"budi suka makan mie ayam\",\n",
    "             \"sering banget makan bakso di tempat ini\",\n",
    "             \"aplikasinya gampang rusak\",\n",
    "             \"susah bange make aplikasi ini\",\n",
    "             \"aplikasi ini sering error\",\n",
    "             \"kadang bisa dipake kadang error kalau make palikasi ini\"]\n",
    "\n",
    "df = pd.DataFrame(sentences, columns=[\"text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "k must be less than or equal to the number of training points",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m topics, probs \u001b[39m=\u001b[39m topic_model\u001b[39m.\u001b[39;49mfit_transform(df[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\bertopic\\_bertopic.py:348\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[1;34m(self, documents, embeddings, y)\u001b[0m\n\u001b[0;32m    345\u001b[0m umap_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reduce_dimensionality(embeddings, y)\n\u001b[0;32m    347\u001b[0m \u001b[39m# Cluster reduced embeddings\u001b[39;00m\n\u001b[1;32m--> 348\u001b[0m documents, probabilities \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cluster_embeddings(umap_embeddings, documents)\n\u001b[0;32m    350\u001b[0m \u001b[39m# Sort and Map Topic IDs by their frequency\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnr_topics:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\bertopic\\_bertopic.py:2317\u001b[0m, in \u001b[0;36mBERTopic._cluster_embeddings\u001b[1;34m(self, umap_embeddings, documents, partial_fit)\u001b[0m\n\u001b[0;32m   2315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopics_ \u001b[39m=\u001b[39m labels\n\u001b[0;32m   2316\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhdbscan_model\u001b[39m.\u001b[39;49mfit(umap_embeddings)\n\u001b[0;32m   2318\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhdbscan_model\u001b[39m.\u001b[39mlabels_\n\u001b[0;32m   2319\u001b[0m     documents[\u001b[39m'\u001b[39m\u001b[39mTopic\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m labels\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\hdbscan\\hdbscan_.py:1209\u001b[0m, in \u001b[0;36mHDBSCAN.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobabilities_ \u001b[39m=\u001b[39m new_probabilities\n\u001b[0;32m   1208\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_data:\n\u001b[1;32m-> 1209\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prediction_data()\n\u001b[0;32m   1211\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\hdbscan\\hdbscan_.py:1248\u001b[0m, in \u001b[0;36mHDBSCAN.generate_prediction_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1245\u001b[0m         warn(\u001b[39m\"\u001b[39m\u001b[39mMetric \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m not supported for prediction data!\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric))\n\u001b[0;32m   1246\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m-> 1248\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prediction_data \u001b[39m=\u001b[39m PredictionData(\n\u001b[0;32m   1249\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raw_data,\n\u001b[0;32m   1250\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcondensed_tree_,\n\u001b[0;32m   1251\u001b[0m         min_samples,\n\u001b[0;32m   1252\u001b[0m         tree_type\u001b[39m=\u001b[39mtree_type,\n\u001b[0;32m   1253\u001b[0m         metric\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric,\n\u001b[0;32m   1254\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metric_kwargs\n\u001b[0;32m   1255\u001b[0m     )\n\u001b[0;32m   1256\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1257\u001b[0m     warn(\n\u001b[0;32m   1258\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot generate prediction data for non-vector\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1259\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mspace inputs -- access to the source data rather\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1260\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthan mere distances is required!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\hdbscan\\prediction.py:103\u001b[0m, in \u001b[0;36mPredictionData.__init__\u001b[1;34m(self, data, condensed_tree, min_samples, tree_type, metric, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat64)\n\u001b[0;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtree \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tree_type_map[tree_type](\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_data,\n\u001b[0;32m    102\u001b[0m                                            metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcore_distances \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree\u001b[39m.\u001b[39;49mquery(data, k\u001b[39m=\u001b[39;49mmin_samples)[\u001b[39m0\u001b[39m][:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_metric \u001b[39m=\u001b[39m DistanceMetric\u001b[39m.\u001b[39mget_metric(metric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    106\u001b[0m selected_clusters \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(condensed_tree\u001b[39m.\u001b[39m_select_clusters())\n",
      "File \u001b[1;32msklearn\\neighbors\\_binary_tree.pxi:1127\u001b[0m, in \u001b[0;36msklearn.neighbors._kd_tree.BinaryTree.query\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: k must be less than or equal to the number of training points"
     ]
    }
   ],
   "source": [
    "topics, probs = topic_model.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0_memakan_makan_buah_suka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1_kadang_error_aplikasi_susah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                           Name\n",
       "0      0      5      0_memakan_makan_buah_suka\n",
       "1      1      4  1_kadang_error_aplikasi_susah"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "060cb0c314b72233d054837cbf7114a14e911c92ed126864d61595f4324447df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
